---
title: Arithmetizing Computable Programs
publish_date: 2024-05-??
category: compilers
image: media/differential-equations-01.png
excerpt: We discuss different ways of encoding computable functions, describing nuances and limitations, before ending on the simplest known method using an intentionally complete operation that's simpler and easier to use than a Turing machine.
---

## Introduction

My task for this post is to describe the simplest way to arithmetize universal computation. To be more precise, I want an operation, $\circ$, such that, for any computable function $f : \mathbb{N} \rightarrow \mathbb{N}$, there exists a number $n_f : \mathbb{N}$ such that $\forall x. n_f \circ x = f(x)$. Such a function is called "intentionally complete". This is not the same as being Turing complete; I'll describe the differences later.

This post is mostly written because I think it's interesting, but, speculatively, this may have implications for future work on zero-knowledge virtual machines. There's a reasonable sense in which a ZK-vm is a cryptographically verifiable, intentionally complete function. Starting with a very simple example of a non-cryptographic functions may lead to valuable insights.

## Traditional Answers

This is, of course, an old problem at this point, and one whose usual answer comes in the form of a universal Turing machine. However, I don't find this very satisfying. Part of that is due to the fundamental inellegance of Turing machines. Despite how often they're mentioned, they're rarely used. Most places where they seem to be used they are merely mentioned. Whenever Turing machines are brought up in white papers, they are almost never acompanied by details of which machine is being used, nor are actual programs mentioned. I think I've seen maybe three or four papers actually give exampkles of Turing machine programs, compered to the over a hundred I've seen mention Turing machines without giving any further details.

Contrast this with the lamdba calculus or the SK combinator calculus; models of computation which are much easier to use. It's rare that mentionings of these aren't acompanied by actual programs.

Part of the difficulty of using Turing machines is the miriad of annoying technicalities that need to be addressed to make use of them. Traditionally, inputs and outputs of Turing machines are infinite tapes. We must define some way to embed a finite input into the tape, and truncate the output to interpret it as finite. Additionally, a Turing machine has two sides, so what does one do with the left side of the tape? Additionally, how is composition supposed to be handled? When composing Turing machines, T and U, are we supposed to craft them into a new Turing machine, or do we run the output of U as the input of T, which isn't a new Turing machine, but a new, meta-construction? If the later, what is the full model of computation?

These questions are not non-starters; I list them to demonstrate how many technicalities are waved away when Turing machines are claimed to be used as a definition for computable function. All these problems can be solved, but this requires making arbitrary choices along the way.

Picking something like the SK combinator calculus or lambda calculus doesn't get us away from all problems either; though the list of outstanding choices is smaller.

Let's go through the details.

Firstly, the SK combinator system has a syntax generated by two constants, S and K, and a binary operation called application, which I'll denote "@". This is a bit more verbose than usual presentations, but I think being explicit here will aid what I plan. The system has two equations defining the evolution of the system.

	- $((S @ x) @ y) @ z = (x @ z) @ (y @ z)$
	- $(K @ y) @ z = y$

SK combinator expressions are not numbers, so we must choose an encoding. A typical encoding is the Church encoding, in which case we may define

	- $0 = K$
	- $1 + x = (S @ (K @ (S @ ((S @ K) @ K)))) @ x$

This works by encoding programs that itterate a function a number of times equal to the number being encoded. If $e_n$ is the encoding of the number $n$, then $(e_n @ z) @ f = f(f(f( ... f(f(z)) ... )))$, where $f$ is repreated $n$ times. To demonstate, we may observe that

	- $(0 @ z) @ f = (K @ z) @ f = z$
	- $((1 + 0) @ z) @ f = 
		(((S @ (K @ (S @ ((S @ K) @ K)))) @ 0) @ z) @ f = 
		(((K @ (S @ ((S @ K) @ K))) @ z) @ (0 @ z)) @ f = 
		((S @ ((S @ K) @ K)) @ (0 @ z)) @ f = 
		(((S @ K) @ K) @ f) @ ((0 @ z) @ f) = 
		(((S @ K) @ K) @ f) @ z =
		(((S @ K) @ K) @ f) @ z =
		((K @ f) @ (K @ f)) @ z=
		f @ z$

By itterating $1 +$, we can build up an encoding for any natural number. Further, it's semidecidable if a program encodes a natural number, since we can just apply the variables $z$ and $f$ and, if it normalizes to an itteration of $f$s, we can just count the $f$s to get back the encoded number.



T f x y = f y x




Further, typical outputs of SK expressions are normalized SK expressions, which are only sometimes interpretable as numbers, according to our encoding. 














